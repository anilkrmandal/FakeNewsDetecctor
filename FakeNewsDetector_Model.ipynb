{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=pd.read_csv(r\"C:\\Users\\anilm\\Downloads\\WELFake_Dataset.csv\\WELFake_Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=data.fillna(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
      
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        
        "def preprocess_text(text):\n",
        
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep only alphanumeric and spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        
        
        "    sentences = sent_tokenize(text)\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    filtered_tokens = [[word for word in token_list if word not in stop_words] for token_list in tokens]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regex_filter(text):\n",
        "    \"\"\"Remove numbers, URLs, emails, and phone numbers.\"\"\"\n",
        "    text = re.sub(r'\\d+', '', text) \n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  \n",
        "    text = re.sub(r'\\S+@\\S+', '', text) \n",
        "    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '', text) \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def fin(text):\n",
        "  text=  re.sub(r'[^\\w\\s]', ' ',text)\n",
        "  text = re.sub(r'[ \\n]+', ' ', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize_text(tokens):\n",
        "    \"\"\"Lemmatize tokens.\"\"\"\n",
        "    lemmatized_tokens = [[lemmatizer.lemmatize(word) for word in token_list] for token_list in tokens]\n",
        "    return lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def full_preprocess_pipeline(text):\n",
       
        "    text = preprocess_text(text)  # Lowercase and remove non-essential characters\n",
        "    text = regex_filter(text)  # Remove numbers, URLs, emails, and phone numbers\n",
        "    text = fin(text)  # Remove special characters and unwanted spaces\n",
        "    tokens = tokenize_and_remove_stopwords(text)  \n",
        "    lemmatized_tokens = lemmatize_text(tokens)  \n",
        "    cleaned_text = ' '.join([' '.join(sentence) for sentence in lemmatized_tokens])\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Apply preprocessing to the 'title' and 'text' columns using the full preprocessing pipeline\n",
        "data['cleaned_text'] = data['text'].apply(full_preprocess_pipeline)\n",
        "data['cleaned_title'] = data['title'].apply(full_preprocess_pipeline)\n",
        "\n",
        "# Optionally, drop the 'Unnamed: 0' column if it's not needed\n",
        "if 'Unnamed: 0' in data.columns:\n",
        "    data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        
        "print(data.head())\n",
        "\n",
        "# Save the processed DataFrame to a new CSV file\n",
        "data.to_csv('processeddata.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_data=pd.read_csv(r\"C:\\Users\\anilm\\Downloads\\processeddata.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(processed_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
       
        "df = pd.read_csv(r\"C:\\Users\\anilm\\Downloads\\processeddata.csv\")\n",
        "\n",
        "# Combine 'cleaned_title' and 'cleaned_text' columns for FastText input\n",
        "df['combined_text'] = df['cleaned_title'].fillna('') + ' ' + df['cleaned_text'].fillna('')\n",
        "\n",
        "# Format labels for FastText (__label__0 for real news, __label__1 for fake news)\n",
        "df['fasttext_format'] = '__label__' + df['label'].astype(str) + ' ' + df['combined_text']\n",
        "\n",
         "train_data, test_data = train_test_split(df['fasttext_format'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the formatted data into text files for FastText\n",
        "with open('fasttext_train.txt', 'w', encoding='utf-8') as train_file:\n",
        "    train_file.write('\\n'.join(train_data))\n",
        "\n",
        "with open('fasttext_test.txt', 'w', encoding='utf-8') as test_file:\n",
        "    test_file.write('\\n'.join(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train a supervised FastText model with specified parameters\n",
        "model = fasttext.train_supervised(\n",
        "    input=\"fasttext_train.txt\",\n",
        "    lr=0.01,  \n",
        "    epoch=50,  \n",
        "    wordNgrams=3,  # Use word n-grams of size 3\n",
        "    dim=300  #  word vectors dimension\n",
        ")\n",
        "\n",
        "# Saving the trained model\n",
        "model.save_model(\"supervised_fasttext_model.bin\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "result = model.test(\"fasttext_test.txt\")\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Number of samples:\", result[0])\n",
        "print(\"Precision:\", result[1])\n",
        "print(\"Recall:\", result[2])\n",
        "\n",
        "# Predicting new examples (optional)\n",
        "examples = [\n",
        "    \"Breaking news: Market crashes after economic policy announcement.\",\n",
        "    \"NASA discovers signs of alien life on Mars!\"\n",
        "]\n",
        "predictions = model.predict(examples)\n",
        "\n",
        "# Display predictions\n",
        "for example, prediction in zip(examples, predictions[0]):\n",
        "    print(f\"Text: {example}\")\n",
        "    print(f\"Prediction: {prediction}\")\n"
      ]
    },
  
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Tokenize the processed texts (each document is a list of words)\n",
        "processed_data1 = [text.split() for text in df['combined_text']]\n",
        "\n",
        "# Create a dictionary and a corpus for LDA\n",
        "dictionary = Dictionary(processed_data1)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_data1]\n",
        "\n",
        "# Train the LDA model\n",
        "num_topics = 3  # Number of topics\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=5)\n",
        "\n",
        "# Extract the dominant topic for each document\n",
        "lda_topics = []\n",
        "for bow in corpus:\n",
        "    topic_distribution = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]  # Get the topic with max probability\n",
        "    lda_topics.append(dominant_topic)\n",
        "\n",
        "# Add the dominant topic as a feature in the DataFrame\n",
        "df['lda_topic'] = lda_topics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine the original text with the LDA topic information\n",
        "df['text_with_topic'] = df['combined_text'] + ' topic_' + df['lda_topic'].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the combined text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['text_with_topic'])\n",
        "sequences = tokenizer.texts_to_sequences(df['text_with_topic'])\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_len = 100  # Maximum length of input sequences\n",
        "X = pad_sequences(sequences, padding='post', maxlen=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from fasttext import load_model\n",
        "\n",
        "# Load your trained FastText model\n",
        "model = load_model(\"supervised_fasttext_model.bin\")\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_dim = 300  # FastText embedding dimension\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_matrix[index] = model.get_word_vector(word)\n",
        "    except KeyError:\n",
        "        # If word not in FastText vocabulary, leave it as zeros\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare labels (binary classification: fake = 1, real = 0)\n",
        "y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Learnable weight for attention mechanism\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Ensure inputs are either a single tensor or a list of [query, value]\n",
        "        if isinstance(inputs, list) and len(inputs) == 2:\n",
        "            query, value = inputs\n",
        "        else:\n",
        "            query, value = inputs, inputs\n",
        "\n",
        "        # Calculate attention scores using the query and value\n",
        "        attention_scores = tf.matmul(value, self.W)  # Shape: (batch_size, seq_length, 1)\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=1)  # Shape: (batch_size, seq_length, 1)\n",
        "\n",
        "        # Apply attention weights to the value tensor\n",
        "        weighted_value = value * attention_weights  # Shape: (batch_size, seq_length, features)\n",
        "\n",
        "        # Reduce across the sequence dimension to obtain the context vector\n",
        "        context_vector = tf.reduce_sum(weighted_value, axis=1)  # Shape: (batch_size, features)\n",
        "        return context_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
     
        "# Create embedding matrix (300 dimensions for FastText)\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if word in model:  # Assuming `model` is your trained FastText embedding\n",
        "        embedding_matrix[index] = model.get_word_vector(word)\n",
        "\n",
        "# Prepare labels\n",
        "y = df['label'].values  # Replace 'label' with your label column\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))  # Adjusts for categorical labels\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# CNN-LSTM Model with Attention\n",
        "cnn_lstm_model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "cnn_lstm_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "                             output_dim=embedding_dim,\n",
        "                             weights=[embedding_matrix],\n",
        "                             input_length=max_len,\n",
        "                             trainable=False))\n",
        "\n",
        "# Conv1D + MaxPooling layers\n",
        "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=4, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# LSTM Layers\n",
        "cnn_lstm_model.add(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01)))\n",
        "cnn_lstm_model.add(LSTM(units=30, return_sequences=True, kernel_regularizer=l2(0.01)))  # Return sequences for Attention\n",
        "\n",
        "# Attention Layer\n",
        "cnn_lstm_model.add(Attention())\n",
        "\n",
        "# Flatten layer\n",
        "cnn_lstm_model.add(Flatten())\n",
        "\n",
        "# Fully connected Dense layers with Dropout\n",
        "cnn_lstm_model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_lstm_model.add(Dropout(0.5))\n",
        "cnn_lstm_model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "cnn_lstm_model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "cnn_lstm_model.add(Dense(y.shape[1], activation='softmax'))  # Multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "cnn_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "cnn_lstm_model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = cnn_lstm_model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = cnn_lstm_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convert numeric class labels to string labels\n",
        "target_names = [str(cls) for cls in encoder.categories_[0]]\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=target_names)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on new examples\n",
        "new_examples = [\n",
        "    \"court order nigeria pay million war victim abuja reuters court monday ordered nigeria pay billion naira million damage victim civil war failure fully demine clear land weaponry end hostility ruling call government pay billion naira directly war victim state put billion naira toward demining construction school court church mosque affected area judge economic community west african state court justice ruled remain large quantity live bomb deprived community farmland since civil war ended sovereign state respect court ruling framework making binding office president muhammadu buhari immediately comment ruling million people died civil war shortlived republic biafra naira\",\n",
        "    \"america give grand piano horse wednesday november lucas wilde america give grand piano horse america given grand piano horse expecting quality tune im particularly looking forward beethoven ninth beamed horse supporter piano enthusiast jay cooper horse never given piano frankly establishment wouldnt allow last change come america change better lot doubter doubter soon silenced graceful note chopin mozart maybe even little richard horse dobbin williams said im really sure whats expected im horse absolutely qualified play piano mean look hoof way general cant even sit chair properly earth anyone think good idea cooper grinned weve made piano great democrat elizabeth king said wanted get pianist lowtomedium standard piano wouldnt thumped anything exciting would perfectly reasonable background music people spoken people wanted horse god bless america get best newsthump story mailbox every friday free currently witterings add\"\n",
        "]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_examples)\n",
        "new_padded = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "\n",
        "# Get predictions\n",
        "predictions = cnn_lstm_model.predict(new_padded)\n",
        "\n",
        "# Interpret predictions\n",
        "for example, prediction in zip(new_examples, predictions):\n",
        "    print(f\"Text: {example}\")\n",
        "    print(f\"Prediction: {'Real News' if prediction[0] < 0.5 else 'Fake News'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install lime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on new examples\n",
        "new_examples = [\n",
        "    \"court order nigeria pay million war victim abuja reuters court monday ordered nigeria pay billion naira million damage victim civil war failure fully demine clear land weaponry end hostility ruling call government pay billion naira directly war victim state put billion naira toward demining construction school court church mosque affected area judge economic community west african state court justice ruled remain large quantity live bomb deprived community farmland since civil war ended sovereign state respect court ruling framework making binding office president muhammadu buhari immediately comment ruling million people died civil war shortlived republic biafra naira\",\n",
        "    \"america give grand piano horse wednesday november lucas wilde america give grand piano horse america given grand piano horse expecting quality tune im particularly looking forward beethoven ninth beamed horse supporter piano enthusiast jay cooper horse never given piano frankly establishment wouldnt allow last change come america change better lot doubter doubter soon silenced graceful note chopin mozart maybe even little richard horse dobbin williams said im really sure whats expected im horse absolutely qualified play piano mean look hoof way general cant even sit chair properly earth anyone think good idea cooper grinned weve made piano great democrat elizabeth king said wanted get pianist lowtomedium standard piano wouldnt thumped anything exciting would perfectly reasonable background music people spoken people wanted horse god bless america get best newsthump story mailbox every friday free currently witterings add\"\n",
        "]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_examples)\n",
        "new_padded = pad_sequences(new_sequences, padding='post', maxlen=max_len)\n",
        "\n",
        "# Get predictions\n",
        "predictions = cnn_lstm_model.predict(new_padded)\n",
        "\n",
        "# Interpret predictions\n",
        "for example, prediction in zip(new_examples, predictions):\n",
        "    print(f\"Text: {example}\")\n",
        "    print(f\"Prediction: {'Real News' if prediction[0] < 0.5 else 'Fake News'}\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Create a function to predict using the CNN-LSTM model\n",
        "def predict_proba(texts):\n",
        "    # Tokenize and pad the texts for prediction\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    X = pad_sequences(sequences, padding='post', maxlen=max_len)\n",
        "    \n",
        "    # Get the model's prediction probabilities (returns probability of \"Fake News\")\n",
        "    proba = cnn_lstm_model.predict(X)\n",
        "    \n",
        "    # Return probabilities for both classes (Real News: 1 - proba, Fake News: proba)\n",
        "    return np.column_stack((1 - proba, proba))\n",
        "\n",
        "# Instantiate the LIME Text Explainer\n",
        "explainer = LimeTextExplainer(class_names=['Real News', 'Fake News'])\n",
        "\n",
        "# Select a sample text for explanation (from test set or any other example)\n",
        "# sample_text = [\"court order nigeria pay million war victim abuja reuters court monday ordered nigeria pay billion naira million damage victim civil war failure fully demine clear land weaponry end hostility ruling call government pay billion naira directly war victim state put billion naira toward demining construction school court church mosque affected area judge economic community west african state court justice ruled remain large quantity live bomb deprived community farmland since civil war ended sovereign state respect court ruling framework making binding office president muhammadu buhari immediately comment ruling million people died civil war shortlived republic biafra naira\"]\n",
        "\n",
        "# Explain the prediction for the sample text\n",
        "explanation = explainer.explain_instance(new_examples[0], predict_proba, num_features=10)\n",
        "# explanation = explainer.explain_instance(new_examples[1], predict_proba, num_features=10)\n",
        "\n",
        "# Visualize the explanation\n",
        "explanation.show_in_notebook()\n",
        "\n",
        "# Alternatively, to print the explanation:\n",
        "print(\"Explanation:\")\n",
        "for feature, weight in explanation.as_list():\n",
        "    print(f\"{feature}: {weight:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have history object from model.fit()\n",
        "history = cnn_lstm_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n",
        "\n",
        "# Plotting loss and accuracy curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions\n",
        "y_pred = cnn_lstm_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Assuming binary classification\n",
        "y_pred_prob = cnn_lstm_model.predict(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_classes, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming the classes are 'Real News' and 'Fake News'\n",
        "target_names = ['Real News', 'Fake News']  # Replace with your actual class names\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=target_names)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=processed_data1, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Calculate perplexity score\n",
        "perplexity_score = lda_model.log_perplexity(corpus)\n",
        "\n",
        "print(\"coherence_score \", coherence_score )\n",
        "print(\"perplexity_score\",perplexity_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gIoolLilTdRB",
        "xy6GnTigNAQy",
        "bNkVvhm5-oSf"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f0fee354943c19a9168220ca3f3255bbea8a3faf2e6b057dd8fedd470257d234"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
